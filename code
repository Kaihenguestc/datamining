#miss data情况
def missing_statistics(df):    
    statitics = pd.DataFrame(df.isnull().sum()).reset_index()
    statitics.columns=['COLUMN NAME',"MISSING VALUES"]
    statitics['TOTAL ROWS'] = df.shape[0]
    statitics['% MISSING'] = round((statitics['MISSING VALUES']/statitics['TOTAL ROWS'])*100,2)
    return statitics
    
# miss data基本填充方法
def impute_NA_with_avg(data,strategy='mean',NA_col=[]):
    """
    replacing the NA with mean/median/most frequent values of that variable. 
    Note it should only be performed over training set and then propagated to test set.
    """
    
    data_copy = data.copy(deep=True)
    for i in NA_col:
        if data_copy[i].isnull().sum()>0:
            if strategy=='mean':
                data_copy[i] = data_copy[i].fillna(data[i].mean())
            elif strategy=='median':
                data_copy[i] = data_copy[i].fillna(data[i].median())
            elif strategy=='mode':
                data_copy[i] = data_copy[i].fillna(data[i].mode()[0])
        else:
            print ("Column %s has no missing" % i)
    return data_copy  
    
#one hot 独热编码=================================================================================
combined_train_test['Sex'] = pd.factorize(combined_train_test['Sex'])[0]
sex_dummies_df = pd.get_dummies(combined_train_test['Sex'], prefix=combined_train_test[['Sex']].columns[0])
combined_train_test = pd.concat([combined_train_test, sex_dummies_df], axis=1)

#------------------------------------------------------------------------------------
le = LabelEncoder()
building.primary_use = le.fit_transform(building.primary_use)
#================================================================================
对目标进行log处理：
def target_log(data):
    data[ycol]=np.log(data[ycol]+1)
    return data

def target_exp(data):
    data[ycol]=np.exp(data[ycol])-1
    return data

#============================
   for col in cate_cols:
        if col == f or col == 'guid':
            continue
        print(col)
        df = df.merge(tmp[col].agg({
          '{}_{}_nunique'.format(f, col): 'nunique',
          '{}_{}_ent'.format(f, col): lambda x: entropy(x.value_counts() / x.shape[0]), # 熵
          '{}_{}_mode'.format(f, col): lambda x: x.mode()[0] # 众数（即出现次数最多的item）
        }).reset_index(), on=f, how='left')
        df = df.merge(df.groupby([f, col], as_index=False)['id'].agg({
          '{}_{}_count'.format(f, col): 'count' # 共现次数
        }), on=[f, col], how='left')
        df['{}_{}_count_ratio'.format(col, f)] = df['{}_{}_count'.format(f, col)] / df[f + '_count'] # 比例偏好
        df['{}_{}_count_ratio'.format(f, col)] = df['{}_{}_count'.format(col, f)] / df[col + '_count'] # 比例偏好
        df['{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df['{}_{}_nunique'.format(f, col)] / df[f + '_count']
        df['{}_{}_mode_count'.format(f, col)] = df['{}_{}_mode'.format(f, col)].map(df['{}_{}_mode'.format(f, col)].value_counts())
        mode_cols.append('{}_{}_mode'.format(f, col))
        print('runtime:', time.time() - t)
 #====================================
