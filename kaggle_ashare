import numpy as np # linear algebra
import pandas as pd
import lightgbm as lgb
from pandas.tseries.holiday import USFederalHolidayCalendar as calendar
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold
from tqdm import tqdm_notebook
import gc

directory = '/kaggle/input/ashrae-energy-prediction'
sample_submission = pd.read_csv(directory+'/sample_submission.csv')
df_train = pd.read_csv(directory+'/train.csv')
weather_train = pd.read_csv(directory+'/weather_train.csv')
#building_metadata = pd.read_csv(directory+'/building_metadata_external.csv')
building_metadata = pd.read_csv('/kaggle/input/building-metadata-external/building_metadata_external.csv')
building_metadata = building_metadata.drop('leed',axis = 1)

print('df_train_shape :' , df_train.shape)
print('weather_train_shape :' , weather_train.shape)
print('building_metadata_shape :' , building_metadata.shape)
#fill_missing_weather=============================================================================
def missing_statistics(df):    
    statitics = pd.DataFrame(df.isnull().sum()).reset_index()
    statitics.columns=['COLUMN NAME',"MISSING VALUES"]
    statitics['TOTAL ROWS'] = df.shape[0]
    statitics['% MISSING'] = round((statitics['MISSING VALUES']/statitics['TOTAL ROWS'])*100,2)
    return statitics
def fill_missing_column(df,filler_df,col):
    null_df = df.loc[df[col].isnull()]
    
    if null_df.empty != True:
        null_df[col] = null_df.apply(lambda x: filler_df.loc[x['site_id']][x['day']][x['month']],axis=1)
        df.loc[null_df.index,col] = null_df[col]
    
    return df
def fill_weather_dataset(weather_df):
    
    # Add new Features
    weather_df["datetime"] = pd.to_datetime(weather_df["timestamp"])
    weather_df["day"] = weather_df["datetime"].dt.day
    weather_df["week"] = weather_df["datetime"].dt.week
    weather_df["month"] = weather_df["datetime"].dt.month
    
    air_temperature_filler = weather_df.groupby(['site_id','day','month'])['air_temperature'].mean()
    weather_df = fill_missing_column(weather_df,air_temperature_filler,'air_temperature')    

    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()
    cloud_coverage_filler = cloud_coverage_filler.fillna(method='ffill')
    weather_df = fill_missing_column(weather_df,cloud_coverage_filler,'cloud_coverage')

    due_temperature_filler = weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean()
    weather_df = fill_missing_column(weather_df,due_temperature_filler,'dew_temperature')

    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()
    sea_level_filler = sea_level_filler.fillna(method='ffill')
    weather_df = fill_missing_column(weather_df,sea_level_filler,'sea_level_pressure')

    wind_direction_filler = weather_df.groupby(['site_id','day','month'])['wind_direction'].mean()
    weather_df = fill_missing_column(weather_df,wind_direction_filler,'wind_direction')

    wind_speed_filler = weather_df.groupby(['site_id','day','month'])['wind_speed'].mean()
    weather_df = fill_missing_column(weather_df,wind_speed_filler,'wind_speed')

    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()
    precip_depth_filler = precip_depth_filler.fillna(method='ffill')
    weather_df = fill_missing_column(weather_df,precip_depth_filler,'precip_depth_1_hr')

    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)
    
    return weather_df
weather_train = fill_weather_dataset(weather_train)
#================================================================================================
df_train = df_train.merge(building_metadata, on='building_id', how='left')
df_train = df_train.merge(weather_train, on=['site_id', 'timestamp'], how='left')
#time aligend====================================================
'''weather_key = ['site_id', 'timestamp']
temp_skeleton = weather_train[weather_key + ['air_temperature']].drop_duplicates(subset=weather_key).sort_values(by=weather_key).copy()

temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')

# create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)
df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)

# Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.
site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)
site_ids_offsets.index.name = 'site_id'

def timestamp_align(df):
    df['offset'] = df.site_id.map(site_ids_offsets)
    df['timestamp_aligned'] = (df.timestamp - pd.to_timedelta(df.offset, unit='H'))
    df['timestamp'] = df['timestamp_aligned']
    del df['timestamp_aligned']
    return df
df_train = timestamp_align(df_train)'''
#===============================================================
del weather_train
gc.collect()
#feature engneering-----------
#drop col
unimportant_col=['sea_level_pressure', 'wind_direction','wind_speed']
df_train.drop(unimportant_col,axis=1,inplace=True)

#time variable
df_train["timestamp"] = pd.to_datetime(df_train["timestamp"])
df_train["hour"] = df_train["timestamp"].dt.hour
df_train["weekend"] = df_train["timestamp"].dt.weekday
df_train["month"] = df_train["timestamp"].dt.month
df_train['year_built'] = df_train['year_built']-1900
df_train['square_feet'] = np.log1p(df_train['square_feet'])

dates_range = pd.date_range(start='2015-12-31', end='2019-01-01')
us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())

df_train['is_holiday'] = (df_train['timestamp'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)
#new_fea==========================================================================
df_train['entertainment_week']=((df_train['is_holiday']==1)&( df_train['primary_use']=='Entertainment/public assembly'))*1
#df_train['site_education'] = ((df_train['site_id']==7)|(df_train['site_id']==11))*1
df_train['site_NO_education'] = (df_train['site_id']==8)
del df_train["timestamp"]

df_train['meter_reading'] = np.log1p(df_train['meter_reading'])
df_train
#Memory reduce
def reduce_mem_usage(df):
    start_mem_usg = df.memory_usage().sum() / 1024**2 
    print("Memory usage of properties dataframe is :",start_mem_usg," MB")
    NAlist = [] # Keeps track of columns that have missing values filled in. 
    for col in df.columns:
        if df[col].dtype != object:  # Exclude strings            
            # Print current column type
            print("******************************")
            print("Column: ",col)
            print("dtype before: ",df[col].dtype)            
            # make variables for Int, max and min
            IsInt = False
            mx = df[col].max()
            mn = df[col].min()
            print("min for this col: ",mn)
            print("max for this col: ",mx)
            # Integer does not support NA, therefore, NA needs to be filled
            if not np.isfinite(df[col]).all(): 
                NAlist.append(col)
                df[col].fillna(mn-1,inplace=True)  
                   
            # test if column can be converted to an integer
            asint = df[col].fillna(0).astype(np.int64)
            result = (df[col] - asint)
            result = result.sum()
            if result > -0.01 and result < 0.01:
                IsInt = True            
            # Make Integer/unsigned Integer datatypes
            if IsInt:
                if mn >= 0:
                    if mx < 255:
                        df[col] = df[col].astype(np.uint8)
                    elif mx < 65535:
                        df[col] = df[col].astype(np.uint16)
                    elif mx < 4294967295:
                        df[col] = df[col].astype(np.uint32)
                    else:
                        df[col] = df[col].astype(np.uint64)
                else:
                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:
                        df[col] = df[col].astype(np.int64)    
            # Make float datatypes 32 bit
            else:
                df[col] = df[col].astype(np.float32)
            
            # Print new column type
            print("dtype after: ",df[col].dtype)
            print("******************************")
    # Print final result
    print("___MEMORY USAGE AFTER COMPLETION:___")
    mem_usg = df.memory_usage().sum() / 1024**2 
    print("Memory usage is: ",mem_usg," MB")
    print("This is ",100*mem_usg/start_mem_usg,"% of the initial size")
    return df, NAlist
df_train ,_ = reduce_mem_usage(df_train)
category = ['month','hour','meter','weekend','primary_use','site_id','building_id']
for col in category :
    df_train[col] = df_train[col].astype('category')
df_train['group'] = df_train['month']
df_train['group'].replace((1,2,3,4,5,6), 1,inplace=True)
df_train['group'].replace((7,8,9,10,11,12), 2, inplace=True)




features = [col for col in df_train.columns if col not in ['meter_reading', 'year', 'month', 'day','group']]
target = 'meter_reading'
NFOLDS = 2
kf = GroupKFold(n_splits=NFOLDS)
models = []
oof = np.zeros(len(df_train))
print('Light GBM Model')
for fold_, (trn_idx, val_idx)  in enumerate(kf.split(df_train, df_train['meter_reading'], groups=df_train['group'])):
        tr_x, tr_y = df_train[features].iloc[trn_idx], df_train[target][trn_idx]
        vl_x, vl_y = df_train[features].iloc[val_idx], df_train[target][val_idx]
        print({'train size':len(tr_x), 'eval size':len(vl_x)})
        print('fold : ',fold_)
        reg= lgb.LGBMRegressor(n_estimators=6000,
                                learning_rate=0.05,
                                feature_fraction=0.7,
                                subsample=0.4,
                                num_leaves=40,
                                metric='rmse')
        reg.fit(tr_x, tr_y, eval_set=[(vl_x, vl_y)],early_stopping_rounds=200,verbose=500)
        oof[val_idx] = reg.predict(df_train.iloc[val_idx][features])
        models.append(reg)
        gc.collect()
    
print('oof_RMSE : ' ,np.sqrt(mean_squared_error(oof, df_train['meter_reading'])))




#predict--------------------
df_test = pd.read_csv('/kaggle/input/ashrae-energy-prediction/test.csv')
weather_test = pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_test.csv')
weather_test = fill_weather_dataset(weather_test)
print('df_test_shape :' , df_test.shape)
print('weather_test_shape :' , weather_test.shape)
df_test = df_test.merge(building_metadata, on='building_id', how='left')
df_test = df_test.merge(weather_test, on=['site_id', 'timestamp'], how='left')
#time aligend==============================
'''weather_key = ['site_id', 'timestamp']
temp_skeleton = weather_test[weather_key + ['air_temperature']].drop_duplicates(subset=weather_key).sort_values(by=weather_key).copy()

temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')

# create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)
df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)

# Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.
site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)
site_ids_offsets.index.name = 'site_id'

def timestamp_align(df):
    df['offset'] = df.site_id.map(site_ids_offsets)
    df['timestamp_aligned'] = (df.timestamp - pd.to_timedelta(df.offset, unit='H'))
    df['timestamp'] = df['timestamp_aligned']
    del df['timestamp_aligned']
    return df
df_test = timestamp_align(df_test)'''
#============================================
del weather_test
del building_metadata
gc.collect()

print('df_test_merge shape: ',df_test.shape)

unimportant_col=['sea_level_pressure', 'wind_direction','wind_speed']
df_test.drop(unimportant_col,axis=1,inplace=True)

#time variable
df_test["timestamp"] = pd.to_datetime(df_test["timestamp"])
df_test["hour"] = df_test["timestamp"].dt.hour
df_test["weekend"] = df_test["timestamp"].dt.weekday
df_test["month"] = df_test["timestamp"].dt.month
df_test['year_built'] = df_test['year_built']-1900
df_test['square_feet'] = np.log1p(df_test['square_feet'])


df_test['is_holiday'] = (df_test['timestamp'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)
#new_fea=======================================================
df_test['entertainment_week']=((df_test['is_holiday']==1)&( df_test['primary_use']=='Entertainment/public assembly'))*1
#df_test['site_education'] = ((df_test['site_id']==7)|(df_test['site_id']==11))*1
df_test['site_NO_education'] = (df_test['site_id']==8)
#=======================================================================
del df_test["timestamp"]
#
df_test ,_ = reduce_mem_usage(df_test)
category = ['month','hour','meter','weekend','primary_use','site_id','building_id']
for col in category :
    df_test[col] = df_test[col].astype('category')
features = [col for col in df_train.columns if col not in ['meter_reading', 'year', 'month', 'day','row_id','group']]
set_size = len(df_test)
iterations = 50
batch_size = set_size // iterations

print(set_size, iterations, batch_size)
assert set_size == iterations * batch_size
meter_reading = []
for i in tqdm_notebook(range(iterations)):
    pos = i*batch_size
    fold_preds = [np.expm1(model.predict(df_test[features].iloc[pos : pos+batch_size])) for model in models]
    meter_reading.extend(np.mean(fold_preds, axis=0))

print(len(meter_reading))
assert len(meter_reading) == set_size
sample_submission['meter_reading'] = np.clip(meter_reading, a_min=0, a_max=None) # clip min at zero
sample_submission.to_csv('submission.csv', index=False)
sample_submission.head(9)
